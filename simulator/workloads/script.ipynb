{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da97add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths_ranges = [(12,128), (600,800)]\n",
    " \n",
    "vcpus_ranges = [(1,2), (6,10)]\n",
    "\n",
    "min_nb_tasks_per_job = 1\n",
    "max_nb_tasks_per_job = 20\n",
    "\n",
    "max_dataset_size_MB = 40960\n",
    "min_dataset_size_MB = 1025\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import json\n",
    "import sys, os\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def arrival_times(nb_nodes=10, lambda_ = 100):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    sum = 0\n",
    "    arrival_times = []\n",
    "    for i in range(nb_nodes):\n",
    "        sum += random.expovariate(lambd=1/lambda_)\n",
    "        arrival_times.append(sum)\n",
    "    return arrival_times\n",
    "\n",
    "def generatSetOfJobs(path_to_file, nb_jobs=200):\n",
    "    global num_jobs, min_nb_tasks_per_job, max_nb_tasks_per_job, min_dataset_size_MB, max_dataset_size_MB, num_jobs\n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    arriving_times_list = arrival_times(nb_nodes=num_jobs, lambda_=10)\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    job_id = 0\n",
    "    while len(jobs) < num_jobs:    \n",
    "\n",
    "        nb_tasks = random.randint(min_nb_tasks_per_job, max_nb_tasks_per_job)\n",
    "        time = random.choice([1, 10, 50, 70, 80, 100]) #random.randint(1, 60)\n",
    "        dataset_size = random.choice([1024, 2048, 5120, 10240, 20480, 40960])#  #random.randint(min_dataset_size_MB, max_dataset_size_MB) \n",
    "        id_dataset = job_id  \n",
    "        \n",
    "        # Create job entry\n",
    "        job = {\n",
    "            \"nb_tasks\": nb_tasks,\n",
    "            \"task_duration\": time,\n",
    "            \"dataset_size\": dataset_size,\n",
    "            \"id_dataset\": id_dataset,\n",
    "            \"arriving_time\": arriving_times_list[job_id],\n",
    "            \"type_of_job\": None\n",
    "        }\n",
    "        job[\"job_id\"]  = job_id\n",
    "        job_id +=1\n",
    "        jobs.append(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        if job[\"task_duration\"] <= 50 and job[\"dataset_size\"] >= 10240:\n",
    "            job[\"type_of_job\"] = 0\n",
    "        elif job[\"task_duration\"] > 50 and job[\"dataset_size\"] <= 10240:\n",
    "            job[\"type_of_job\"] = 1\n",
    "        elif job[\"task_duration\"] > 50 and job[\"dataset_size\"] > 10240:\n",
    "            job[\"type_of_job\"] = 2\n",
    "        else:\n",
    "            job[\"type_of_job\"] = 3\n",
    "\n",
    "    with open(path_to_file, \"w\") as file:\n",
    "        json.dump(jobs, file, indent=4)\n",
    "\n",
    "\n",
    "    return jobs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def generateHeterogeneousInfrastructureEquilibre(nb_node, path):\n",
    "    \"\"\"Generate a heterogeneous infrastructure with random bandwidth for each compute node and return a pandas DataFrame.\"\"\"\n",
    "    random.seed(42)\n",
    "    categories = 4\n",
    "    \n",
    "    taille_categories = {\n",
    "        0: 0.24,\n",
    "        1: 0.26,\n",
    "        2: 0.24,\n",
    "        3: 0.26\n",
    "    }\n",
    "    \n",
    "    cats = {\n",
    "        0: {\"VCPU\": (1, 2.1), \"BW\": (12, 128)},\n",
    "        1: {\"VCPU\": (6, 10),  \"BW\": (12, 128)},\n",
    "        2: {\"VCPU\": (1, 2.1), \"BW\": (600, 800)},\n",
    "        3: {\"VCPU\": (6, 10),  \"BW\": (600, 800)},\n",
    "    }\n",
    "\n",
    "    nodes_config = []\n",
    "\n",
    "    for cat in range(categories):\n",
    "        nodes_needed = int(taille_categories[cat] * nb_node)\n",
    "        for _ in range(nodes_needed):\n",
    "            vcpu = random.uniform(*cats[cat][\"VCPU\"])\n",
    "            bw = random.randint(*cats[cat][\"BW\"])\n",
    "            energy = random.uniform(0.1, 2.1)\n",
    "            nodes_config.append([bw, vcpu, energy])\n",
    "\n",
    "    # Conversion en DataFrame\n",
    "    df = pd.DataFrame(nodes_config, columns=[\"bandwidth\", \"computation_nodes\", \"energy_consumption\"])\n",
    "    x = df.to_csv(path, index=False)\n",
    "\n",
    "    return nodes_config\n",
    "\n",
    "\n",
    "def translatToMinizinc(nodes_config, jobs, path_to_file):\n",
    "    \n",
    "    params = {\n",
    "        \"nb_nodes\": len(nodes_config),\n",
    "        \"nb_data\": len(jobs),   \n",
    "        \"makespan\": 221506,\n",
    "        \"data_sizes\": [job[\"dataset_size\"] for job in jobs],\n",
    "        \"work_duration\": [job[\"task_duration\"] for job in jobs],\n",
    "        \"bandwidths\": [node[0] for node in nodes_config],\n",
    "        \"cpus\": [node[1] * 1 for node in nodes_config],\n",
    "        \"energy_consumptions\": [[]],\n",
    "        \"nb_works\": [job[\"nb_tasks\"] for job in jobs],\n",
    "        \"node_free_timespan\": [],\n",
    "        \"starting_times\": [int(job[\"arriving_time\"]) for job in jobs],\n",
    "    }\n",
    "\n",
    "        # ---- Convert to DZN ----\n",
    "    dzn_path = path_to_file \n",
    "    with open(dzn_path, \"w\") as d:\n",
    "        for key, value in params.items():\n",
    "            if key == \"transfers_time\":\n",
    "                d.write(\"transfers_time = [\")\n",
    "                for row in value:\n",
    "                    d.write(\"|\" + \", \".join(map(str, row)) + \",\")\n",
    "                d.write(\"|];\\n\")\n",
    "            elif isinstance(value, list):\n",
    "                d.write(f\"{key} = {value};\\n\")\n",
    "            else:\n",
    "                d.write(f\"{key} = {value};\\n\")\n",
    "\n",
    "insts = [(5,10),(10,50),(20,50),(20,100),(50,100)]\n",
    "\n",
    "for num_jobs, num_nodes in insts:\n",
    "    path = f'/Users/cherif/Documents/Traveaux/simulator-for-CSP-model/simulator/workloads/GeneratedJobs/inst1-{num_jobs}j-{num_nodes}Nodes/'\n",
    "    nodes_config = generateHeterogeneousInfrastructureEquilibre(num_nodes, path+\"infrastructure.csv\" )\n",
    "    jobs = generatSetOfJobs(path+\"jobs.json\", nb_jobs=num_jobs)\n",
    "    translatToMinizinc( nodes_config, jobs, path+f\"data{num_jobs}_{num_nodes}.dzn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b67c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102, 104, 136, 161, 295, 408, 630, 640, 694, 697, 722, 792, 795, 817, 922, 1001, 1026, 1115, 1281, 1281, 1445, 1565, 1607, 1623, 1939, 1980, 1989, 2000, 2188, 2280, 2445, 2576, 2652, 3014, 3062, 3142, 3319, 3415, 3613, 3699, 3821, 3826, 3852, 3886, 3894, 3921, 3931, 3964, 4065, 4110]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print([int(x) for x in arrival_times(50, 100)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5da685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
